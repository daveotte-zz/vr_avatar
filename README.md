# vr_avatar

I use motion capture data to train a machine learning model to predict where the elbow should be given only the location of the head and hands are known. The idea is to demonstrate that AI can be used to articulate a VR Avatar. In the video I show some results.

This is a stand alone yQt application that was developed and runs in Ubuntu. It sits on top of a pipeline used to process the motion capture data.

[![Watch the video](https://github.com/daveotte/vr_avatar/blob/master/vr_avatar.png)](https://www.youtube.com/watch?v=mwAxl_vj8_0&t=2s)
